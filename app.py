import streamlit as st
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit.components.v1 as components

# ====== TH√äM CAPTUM (Integrated Gradients) ======
try:
    from captum.attr import LayerIntegratedGradients
except ImportError:
    raise ImportError(
        "B·∫°n c·∫ßn c√†i captum tr∆∞·ªõc:\n"
        "    pip install captum"
    )

# ============================================================
# 1. C·∫§U H√åNH TRANG
# ============================================================
st.set_page_config(
    page_title="Fake News Detector",
    page_icon="üïµÔ∏è",
    layout="centered"
)

st.markdown("""
<style>
.stButton>button {
    width: 100%;
    border-radius: 5px;
    height: 3em;
}
</style>
""", unsafe_allow_html=True)

MODEL_PATH = r"D:\University\year_4\Semester_1\Natural Language Processing\my_fakenews_app\distilbert_final"
MAX_LENGTH = 128


# ============================================================
# 2. LOAD MODEL (CPU)
# ============================================================
@st.cache_resource
def load_model():
    device = torch.device("cpu")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()

    return tokenizer, model, device


tokenizer, model, device = load_model()


# ============================================================
# 3. H√ÄM D·ª∞ ƒêO√ÅN
# ============================================================
def predict_proba(text: str):
    enc = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        logits = model(**enc).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
    return probs, enc


# ============================================================
# 4. GI·∫¢I TH√çCH B·∫∞NG INTEGRATED GRADIENTS (IG)
# ============================================================
def explain_with_ig(text: str):
    # X√°c su·∫•t g·ªëc
    base_probs, enc = predict_proba(text)

    input_ids = enc["input_ids"]        # (1, L)
    attention_mask = enc["attention_mask"]

    # H√†m forward tr·∫£ v·ªÅ x√°c su·∫•t l·ªõp Fake News (index 1)
    def forward_func(input_ids_, attention_mask_):
        outputs = model(input_ids=input_ids_, attention_mask=attention_mask_)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        # tr·∫£ v·ªÅ batch vector [batch_size], m·ªói ph·∫ßn t·ª≠ l√† prob c·ªßa class 1
        return probs[:, 1]

    # LayerIntegratedGradients tr√™n embedding layer
    lig = LayerIntegratedGradients(forward_func, model.get_input_embeddings())

    # Baseline: to√†n b·ªô l√† token [PAD] (n·∫øu kh√¥ng c√≥ th√¨ d√πng CLS)
    pad_id = tokenizer.pad_token_id
    if pad_id is None:
        pad_id = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else 0

    baseline_ids = torch.full_like(input_ids, pad_id)

    attributions, delta = lig.attribute(
        inputs=input_ids,
        baselines=baseline_ids,
        additional_forward_args=(attention_mask,),
        n_steps=50,
        return_convergence_delta=True
    )

    # G·ªôp attribution tr√™n chi·ªÅu embedding ‚Üí 1 gi√° tr·ªã / token
    attributions = attributions.sum(dim=-1).squeeze(0)  # (L,)
    attributions = attributions.detach().cpu().numpy().tolist()

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    return tokens, attributions, base_probs


def build_html_explanation(tokens, importances):
    # B·ªè ph·∫ßn padding ph√≠a sau
    cleaned_tokens = []
    cleaned_importances = []
    for tok, imp in zip(tokens, importances):
        if tok == tokenizer.pad_token:
            break
        cleaned_tokens.append(tok)
        cleaned_importances.append(imp)

    tokens = cleaned_tokens
    importances = cleaned_importances

    if not importances:
        return "<p>Kh√¥ng t·∫°o ƒë∆∞·ª£c gi·∫£i th√≠ch.</p>"

    # Chu·∫©n h√≥a theo tr·ªã tuy·ªát ƒë·ªëi l·ªõn nh·∫•t
    max_abs = max(abs(x) for x in importances) or 1e-6

    spans = []
    for tok, imp in zip(tokens, importances):
        # B·ªè lu√¥n [CLS], [SEP], [PAD] n·∫øu c√≤n
        if tok in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:
            continue

        strength = min(1.0, max(0.15, abs(imp) / max_abs))

        if imp > 0:
            # ƒê·∫©y v·ªÅ Fake News ‚Üí ƒë·ªè
            color = f"rgba(255, 0, 0, {strength:.2f})"
        elif imp < 0:
            # ƒê·∫©y v·ªÅ Real News ‚Üí xanh
            color = f"rgba(0, 120, 255, {strength:.2f})"
        else:
            color = "rgba(0,0,0,0)"

        display_tok = tok
        if tok.startswith("##"):
            display_tok = tok[2:]
            space = ""
        else:
            space = " "

        span = (
            f"<span style='background-color:{color}; "
            f"padding:2px 3px; border-radius:3px; margin:1px;'>{display_tok}</span>{space}"
        )
        spans.append(span)

    html = f"""
    <div style="font-family: monospace; line-height: 1.8; font-size: 14px;">
        {''.join(spans)}
    </div>
    """

    return html


# ============================================================
# 5. GIAO DI·ªÜN
# ============================================================
st.title("üïµÔ∏è Ph√°t hi·ªán Tin Gi·∫£ & Gi·∫£i Th√≠ch")
st.markdown("---")
st.write("Nh·∫≠p n·ªôi dung c·∫ßn ki·ªÉm tra b·∫±ng **Ti·∫øng Anh!!!**")

input_text = st.text_area(
    "N·ªôi dung tin t·ª©c",
    height=150,
    placeholder="Paste your text here..."
)


# ============================================================
# 6. BUTTON PH√ÇN T√çCH
# ============================================================
if st.button("üîç Ph√¢n t√≠ch & Gi·∫£i th√≠ch", type="primary"):

    if not input_text.strip():
        st.warning("‚ö† Vui l√≤ng nh·∫≠p n·ªôi dung!")
        st.stop()

    # 6.1. D·ª± ƒëo√°n
    with st.spinner("üîé AI ƒëang ƒë·ªçc hi·ªÉu vƒÉn b·∫£n..."):
        probs, _ = predict_proba(input_text)
        real_score, fake_score = probs
        pred_idx = int(np.argmax(probs))

    st.markdown("### 1. K·∫øt qu·∫£ D·ª± ƒëo√°n")
    col1, col2 = st.columns(2)
    col1.metric("Real News", f"{real_score:.1%}")
    col2.metric("Fake News", f"{fake_score:.1%}")

    if pred_idx == 1:
        st.error(f"üü• K·∫øt lu·∫≠n: **FAKE NEWS** ({fake_score:.1%})")
    else:
        st.success(f"üü© K·∫øt lu·∫≠n: **REAL NEWS** ({real_score:.1%})")

    # 6.2. Gi·∫£i th√≠ch b·∫±ng IG
    st.markdown("---")
    st.markdown("### 2. T·∫°i sao m√¥ h√¨nh d·ª± ƒëo√°n nh∆∞ v·∫≠y?")
    st.info("üî¥ ƒê·ªè = T·ª´ l√†m tƒÉng x√°c su·∫•t Fake News ‚Äî üîµ Xanh = T·ª´ k√©o v·ªÅ Real News")

    with st.spinner("üß† ƒêang t√≠nh m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa t·ª´ng t·ª´ (Integrated Gradients)..."):
        try:
            tokens, importances, _ = explain_with_ig(input_text)
            html = build_html_explanation(tokens, importances)
            components.html(html, height=400, scrolling=True)
        except Exception as e:
            st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o gi·∫£i th√≠ch: {e}")


# ============================================================
# 7. FOOTER
# ============================================================
st.markdown("---")
st.caption("‚ú® Powered by DistilBERT + Integrated Gradients (Captum) + Streamlit")